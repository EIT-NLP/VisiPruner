{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llava-visipruner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/opt/conda/envs/llava-visipruner/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:56<00:00, 88.25s/it] \n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import TextStreamer\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "IMAGE_PATH = \"/code/yingqi/LLaVA_visiPruner/v1_68.png\"\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image\n",
    "\n",
    "if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "    sys.argv = sys.argv[:1]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--model-path\", type=str, default=\"/code/yingqi/models/liuhaotian/llava-v1.5-7b\")\n",
    "parser.add_argument(\"--model-base\", type=str, default=None)\n",
    "parser.add_argument(\"--image-file\", type=str, default=IMAGE_PATH)\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda\")\n",
    "parser.add_argument(\"--conv-mode\", type=str, default=\"vicuna_v1\")\n",
    "parser.add_argument(\"--answer-with-sentence\", type=bool, default=False)\n",
    "parser.add_argument(\"--num-chunks\", type=int, default=1)\n",
    "parser.add_argument(\"--chunk-idx\", type=int, default=0)\n",
    "parser.add_argument(\"--temperature\", type=float, default=0)\n",
    "parser.add_argument(\"--top_p\", type=float, default=None)\n",
    "parser.add_argument(\"--num_beams\", type=int, default=1)\n",
    "parser.add_argument(\"--max_new_tokens\", type=int, default=128)\n",
    "parser.add_argument(\"--load-8bit\", action=\"store_true\")\n",
    "parser.add_argument(\"--load-4bit\", action=\"store_true\")\n",
    "parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "model_name = get_model_name_from_path(args.model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name, args.load_8bit, args.load_4bit, device=args.device)\n",
    "\n",
    "if \"llama-2\" in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"mistral\" in model_name.lower():\n",
    "    conv_mode = \"mistral_instruct\"\n",
    "elif \"v1.6-34b\" in model_name.lower():\n",
    "    conv_mode = \"chatml_direct\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "lm_head = model.lm_head\n",
    "norm = model.model.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] the auto inferred conversation mode is llava_v1, while `--conv-mode` is vicuna_v1, using vicuna_v1\n",
      "ASSISTANT: 4\n"
     ]
    }
   ],
   "source": [
    "if args.conv_mode is not None and conv_mode != args.conv_mode:\n",
    "    print('[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}'.format(conv_mode, args.conv_mode, args.conv_mode))\n",
    "else:\n",
    "    args.conv_mode = conv_mode\n",
    "\n",
    "conv = conv_templates[args.conv_mode].copy()\n",
    "if \"mpt\" in model_name.lower():\n",
    "    roles = ('user', 'assistant')\n",
    "else:\n",
    "    roles = conv.roles\n",
    "\n",
    "image = load_image(\"/code/yingqi/LLaVA_visiPruner/images/v1_73.jpg\")\n",
    "image_size = image.size\n",
    "image_tensor = process_images([image], image_processor, model.config)\n",
    "if type(image_tensor) is list:\n",
    "    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n",
    "else:\n",
    "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
    "\n",
    "inp = \"How many cars?\\nAnswer using a single word or a phrase.\"\n",
    "\n",
    "print(f\"{roles[1]}: \", end=\"\")\n",
    "\n",
    "if image is not None:\n",
    "    # first message\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "    else:\n",
    "        inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "    image = None\n",
    "\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor.to(dtype=torch.float16, device='cuda', non_blocking=True),\n",
    "        image_sizes=[image_size],\n",
    "        do_sample=True if args.temperature > 0 else False,\n",
    "        temperature=args.temperature,\n",
    "        top_p=args.top_p,\n",
    "        num_beams=args.num_beams,\n",
    "        max_new_tokens=128,\n",
    "        streamer=streamer,\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 top tokens: ['Portail', 'ensoort', 'iemann', 'weis', '~[']\n",
      "Layer 1 top tokens: ['Portail', 'archivi', 'embros', 'пута', 'sterd']\n",
      "Layer 2 top tokens: ['sterd', 'Portail', 'Архив', 'férences', 'actér']\n",
      "Layer 3 top tokens: ['sterd', 'férences', 'paździer', 'nahm', 'Архив']\n",
      "Layer 4 top tokens: ['sterd', 'publique', 'paździer', 'Архив', 'Portail']\n",
      "Layer 5 top tokens: ['sterd', 'paździer', 'strij', 'sierp', 'Außer']\n",
      "Layer 6 top tokens: ['sterd', 'paździer', 'sierp', 'gior', 'Gemeins']\n",
      "Layer 7 top tokens: ['sterd', 'typen', 'gior', 'othèque', 'dienst']\n",
      "Layer 8 top tokens: ['penas', 'евич', 'intrag', '桥', 'kazy']\n",
      "Layer 9 top tokens: ['sterd', 'shared', 'geldig', 'none', 'penas']\n",
      "Layer 10 top tokens: ['number', 'zero', 'sterd', 'Zero', 'cyc']\n",
      "Layer 11 top tokens: ['number', 'stor', 'VS', 'gem', 'cyc']\n",
      "Layer 12 top tokens: ['gem', 'égl', 'zero', 'VS', 'stor']\n",
      "Layer 13 top tokens: ['number', 'rita', 'zero', 'arden', 'none']\n",
      "Layer 14 top tokens: ['number', 'arden', 'rita', 'ubre', 'nim']\n",
      "Layer 15 top tokens: ['number', 'arden', 'none', 'rita', 'Number']\n",
      "Layer 16 top tokens: ['number', 'arden', 'rita', 'estaven', 'Number']\n",
      "Layer 17 top tokens: ['number', 'estaven', 'three', 'four', 'érique']\n",
      "Layer 18 top tokens: ['four', 'number', 'three', 'Ges', 'érique']\n",
      "Layer 19 top tokens: ['four', 'three', 'number', 'Ges', 'dozen']\n",
      "Layer 20 top tokens: ['four', 'three', 'number', 'dozen', 'five']\n",
      "Layer 21 top tokens: ['four', 'three', 'several', 'dozen', 'two']\n",
      "Layer 22 top tokens: ['four', 'three', 'several', 'dozen', 'two']\n",
      "Layer 23 top tokens: ['four', 'three', 'five', 'several', 'six']\n",
      "Layer 24 top tokens: ['four', 'three', 'five', 'several', 'six']\n",
      "Layer 25 top tokens: ['four', 'three', 'five', 'several', 'six']\n",
      "Layer 26 top tokens: ['four', 'three', 'five', 'several', 'six']\n",
      "Layer 27 top tokens: ['four', 'three', 'several', 'five', 'many']\n",
      "Layer 28 top tokens: ['four', 'several', 'three', 'five', 'many']\n",
      "Layer 29 top tokens: ['several', '', 'many', 'Several', 'four']\n",
      "Layer 30 top tokens: ['', 'several', 'Several', 'many', 'Many']\n",
      "Layer 31 top tokens: ['', '(', '<0x0A>', 'in', 'Many']\n",
      "Layer 32 top tokens: ['', 'Several', 'Many', 'Lot', 'Multiple']\n"
     ]
    }
   ],
   "source": [
    "# Please be noted: the hidden states of the last layer is recorded after the norm layer, while the states of previous layers are not.\n",
    "for layer_idx, layer in enumerate(prefill_hidden_states):\n",
    "    token_idx = -1\n",
    "    logits = lm_head(norm(layer[0,token_idx,:])) if layer_idx != 31 else lm_head(layer[0,token_idx,:])\n",
    "    probs, token_ids = logits.topk(5)\n",
    "    decoded = [tokenizer.decode(token_id) for token_id in token_ids]\n",
    "    print(f\"Layer {layer_idx} top 5 tokens: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-visipruner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
